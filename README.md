# keras-optimizer
Keras-Optimizer  --  collection of optimizers for keras

 ## Supported Optimizers

| https://arxiv.org/abs/2106.11514 |AdaMomentum| Rethinking Adam: A Twofold Exponential Moving Average Approach |
|:--------------------------------:|:-----------:|:-----------------------------------------------------------------------------------------------------:|
| https://arxiv.org/abs/2101.11075 |MadGrad| Adaptivity without Compromise: A Momentumized, Adaptive, Dual Averaged Gradient Method for Stochastic Optimization | 
| https://arxiv.org/abs/2010.07468 |AdaBelief| AdaBelief Optimizer: Adapting Stepsizes by the Belief in Observed Gradients |
| https://arxiv.org/abs/2009.13586 |Apollo| Apollo: An Adaptive Parameter-wise Diagonal Quasi-Newton Method for Nonconvex Stochastic Optimization |
| https://arxiv.org/abs/2006.13484 |Lans| Accelerated Large Batch Optimization of BERT Pretraining in 54 minutes | 
| https://arxiv.org/abs/2003.07422 |Rm3| Weak and Strong Gradient Directions: Explaining Memorization, Generalization, and Hardness of Examples at Scale | 
| https://arxiv.org/abs/2002.03432 |Fromage| On the distance between two neural networks and the stability of learning |
| https://arxiv.org/abs/1908.03265 |RectifiedAdam| On The Variance Of The Adaptive Learning Rate And Beyond | 
| https://arxiv.org/abs/1904.00962 |LAMB| Large Batch Optimization for Deep Learning: Training BERT in 76 minutes | 
| https://arxiv.org/abs/1804.04235 |AdaFactor| Adafactor: Adaptive Learning Rates with Sublinear Memory Cost |
 
 ## Warning: some optimizer implementations are not strictly verified.
